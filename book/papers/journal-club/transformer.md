# Transformer Framework


## Attention

Attention can be described as a mapping of a query and a set of key-value pairs to n output, where all the above are vectors. The output is  weighted sum of the vlues, where the weights are computed by  compataability function of the query with the corresponding key. 

## Look Up

* Sequence transduction

* Atention mechanisms

* Multi-head attention

* Residual connection

* 
